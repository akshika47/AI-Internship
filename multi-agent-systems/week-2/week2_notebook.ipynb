{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# üß† Week 2: From Graphs to Real Agents\n",
    "\n",
    "## LangGraph Deep Dive ‚Äî Stateful Multi-Agent Systems\n",
    "\n",
    "Welcome to Week 2! Last week you drew graphs, built nodes, and wired edges. This week, your graphs learn to **think, act, and remember**.\n",
    "\n",
    "> **üìö Official Documentation:** This notebook follows the [LangGraph Official Documentation](https://docs.langchain.com/oss/python/langgraph/overview). All APIs and patterns used here are based on the current LangGraph specification.\n",
    "\n",
    "---\n",
    "\n",
    "## üöÄ What You'll Learn Today\n",
    "\n",
    "1. **Typed State** ‚Äî Design state like an engineer with proper types and reducers\n",
    "2. **ReAct Pattern** ‚Äî The core loop that turns graphs into agents (Reason ‚Üí Act ‚Üí Observe ‚Üí Repeat)\n",
    "3. **Conditional Edges & Termination** ‚Äî How agents decide what happens next\n",
    "4. **Checkpointing** ‚Äî Give your agents memory and replay capabilities\n",
    "5. **Live Build** ‚Äî Build a Meeting Prep Agent from scratch\n",
    "\n",
    "### üéØ By the End of This Notebook\n",
    "\n",
    "- ‚úÖ Understand typed state and reducers\n",
    "- ‚úÖ Implement the ReAct pattern in LangGraph\n",
    "- ‚úÖ Build agents with memory using checkpointing\n",
    "- ‚úÖ Create a working Meeting Prep Agent\n",
    "- ‚úÖ Be ready for multi-agent systems in Week 3\n",
    "\n",
    "---\n",
    "\n",
    "### üìñ How to Use This Notebook\n",
    "\n",
    "1. **Run cells in order** - Each cell builds on the previous one\n",
    "2. **Read the markdown cells** - They contain important explanations\n",
    "3. **Experiment** - Try modifying the code to see what happens\n",
    "4. **Complete the challenge** - Build your own extended agent\n",
    "\n",
    "**Ready? Let's start by installing dependencies!**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üîß Setup & Installation\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install dependencies (run once)\n",
    "# If you have requirements.txt in the same directory, you can use:\n",
    "# %pip install -r requirements.txt\n",
    "\n",
    "# Otherwise, install individually:\n",
    "%pip install langgraph langchain-core langchain-openai langchain ipython python-dotenv tavily-python\n",
    "\n",
    "# Verify installation\n",
    "import sys\n",
    "print(f\"Python version: {sys.version}\")\n",
    "print(\"‚úÖ Dependencies installed successfully!\")\n",
    "print(\"üí° Make sure you have a .env file with OPENAI_API_KEY for the agent to work!\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## üì¶ Imports\n",
    "\n",
    "Let's import all the essential components we'll need:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Core imports\n",
    "import operator\n",
    "from typing import Annotated, Literal\n",
    "from typing_extensions import TypedDict\n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "from datetime import datetime\n",
    "\n",
    "# Load environment variables from .env file\n",
    "load_dotenv()\n",
    "\n",
    "# LangGraph imports\n",
    "from langgraph.graph import StateGraph, START, END\n",
    "from langgraph.checkpoint.memory import MemorySaver\n",
    "\n",
    "# LangChain imports\n",
    "from langchain_core.messages import AIMessage, HumanMessage, ToolMessage\n",
    "from langchain_core.tools import tool\n",
    "from langchain_openai import ChatOpenAI\n",
    "\n",
    "# Tavily Search import\n",
    "try:\n",
    "    from tavily import TavilyClient\n",
    "    TAVILY_AVAILABLE = True\n",
    "except ImportError:\n",
    "    TAVILY_AVAILABLE = False\n",
    "    print(\"‚ö†Ô∏è  Tavily not installed. Install with: pip install tavily-python\")\n",
    "\n",
    "# Verify API keys are loaded\n",
    "api_key = os.getenv(\"OPENAI_API_KEY\")\n",
    "tavily_key = os.getenv(\"TAVILY_API_KEY\")\n",
    "\n",
    "if api_key:\n",
    "    print(\"‚úÖ OpenAI API key loaded from .env\")\n",
    "else:\n",
    "    print(\"‚ö†Ô∏è  Warning: OPENAI_API_KEY not found in .env file\")\n",
    "\n",
    "if tavily_key and TAVILY_AVAILABLE:\n",
    "    print(\"‚úÖ Tavily API key loaded from .env\")\n",
    "    tavily_client = TavilyClient(api_key=tavily_key)\n",
    "else:\n",
    "    print(\"‚ö†Ô∏è  Warning: TAVILY_API_KEY not found or Tavily not installed\")\n",
    "    tavily_client = None\n",
    "\n",
    "print(\"‚úÖ All imports successful!\")\n",
    "print(\"üìö Using official LangGraph API from: https://docs.langchain.com/oss/python/langgraph/overview\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# üìì Concept 1: Typed State\n",
    "\n",
    "## Why State Design Matters\n",
    "\n",
    "In Week 1, you learned about TypedDict and basic state design. This week, we're going **deeper** into state design for production agents with multiple fields, different reducer patterns, and more complex state management.\n",
    "\n",
    "### Building on Week 1\n",
    "\n",
    "Week 1 introduced:\n",
    "- `TypedDict` for state structure\n",
    "- `Annotated[list, operator.add]` for append-only fields\n",
    "- Basic state management\n",
    "\n",
    "Week 2 expands this with:\n",
    "- **Multiple state fields** with different update patterns\n",
    "- **ReAct trace tracking** (`steps` field)\n",
    "- **Final answer handling** (overwrite vs append)\n",
    "- **More complex state schemas** for real agents\n",
    "\n",
    "### The Solution: TypedDict + Reducers\n",
    "\n",
    "**TypedDict** defines the shape of your state.\n",
    "**Annotated + Reducers** control how updates are merged.\n",
    "\n",
    "### The Rule\n",
    "\n",
    "Nodes either **append** to `messages`/`steps` OR **set** `final_answer`. Never both.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a proper typed state for an agent\n",
    "class AgentState(TypedDict):\n",
    "    # Append-only fields (use operator.add reducer)\n",
    "    messages: Annotated[list, operator.add]  # Conversation history\n",
    "    steps: Annotated[list, operator.add]     # ReAct trace: action + observation pairs\n",
    "    \n",
    "    # Overwrite fields (no reducer)\n",
    "    final_answer: str  # Completed output (set once)\n",
    "\n",
    "print(\"‚úÖ AgentState defined!\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Understanding Reducers\n",
    "\n",
    "**Reducers** determine how state updates are merged:\n",
    "\n",
    "| Field | Reducer | Behavior |\n",
    "|-------|---------|----------|\n",
    "| `messages` | `operator.add` | **Append** new messages to existing list |\n",
    "| `steps` | `operator.add` | **Append** new steps to existing list |\n",
    "| `final_answer` | None | **Overwrite** the field completely |\n",
    "\n",
    "### Why This Matters\n",
    "\n",
    "```python\n",
    "# Without reducer (BAD):\n",
    "# Node 1 returns: {\"steps\": [step_1]}\n",
    "# State: [step_1]\n",
    "# Node 2 returns: {\"steps\": [step_2]}\n",
    "# State: [step_2] ‚Üê step_1 GONE! üò±\n",
    "\n",
    "# With operator.add (GOOD):\n",
    "# Node 1 returns: {\"steps\": [step_1]}\n",
    "# State: [step_1]\n",
    "# Node 2 returns: {\"steps\": [step_2]}\n",
    "# State: [step_1, step_2] ‚úì\n",
    "```\n",
    "\n",
    "**Common gotcha:** If you forget the reducer, your second node erases everything the first node wrote. This is the #1 LangGraph debugging headache!\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Understanding State Updates in Practice\n",
    "\n",
    "The key difference is how state updates are merged. Let's see this in action with a simple example:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Quick demonstration: Why reducers matter\n",
    "# This shows the difference between append (with reducer) and overwrite (without reducer)\n",
    "\n",
    "# Simulate state updates from two nodes\n",
    "state = {\n",
    "    \"messages\": [],\n",
    "    \"steps\": [],\n",
    "    \"final_answer\": \"\"\n",
    "}\n",
    "\n",
    "# Node 1 returns an update\n",
    "node1_update = {\"messages\": [AIMessage(\"Hello!\")], \"steps\": [{\"type\": \"reason\", \"content\": \"Thinking...\"}]}\n",
    "# With reducer: messages get appended\n",
    "state[\"messages\"] = operator.add(state[\"messages\"], node1_update[\"messages\"])\n",
    "state[\"steps\"] = operator.add(state[\"steps\"], node1_update[\"steps\"])\n",
    "print(f\"After Node 1:\")\n",
    "print(f\"  Messages: {len(state['messages'])} (content: {[m.content for m in state['messages']]})\")\n",
    "print(f\"  Steps: {len(state['steps'])}\")\n",
    "\n",
    "# Node 2 returns another update\n",
    "node2_update = {\"messages\": [AIMessage(\"How can I help?\")], \"steps\": [{\"type\": \"act\", \"tool\": \"lookup\"}]}\n",
    "# With reducer: messages get appended (not replaced!)\n",
    "state[\"messages\"] = operator.add(state[\"messages\"], node2_update[\"messages\"])\n",
    "state[\"steps\"] = operator.add(state[\"steps\"], node2_update[\"steps\"])\n",
    "print(f\"\\nAfter Node 2:\")\n",
    "print(f\"  Messages: {len(state['messages'])} (content: {[m.content for m in state['messages']]})\")\n",
    "print(f\"  Steps: {len(state['steps'])}\")\n",
    "print(f\"  ‚úÖ Both nodes' updates preserved!\")\n",
    "\n",
    "# Final answer: overwrite (no reducer)\n",
    "state[\"final_answer\"] = \"Original First answer\"\n",
    "state[\"final_answer\"] = \"Final answer\"  # Overwrites previous\n",
    "print(f\"\\nFinal answer: {state['final_answer']}\")\n",
    "print(f\"  ‚úÖ Overwrites previous value (no reducer)\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# üîÑ Concept 2: The ReAct Pattern\n",
    "\n",
    "## Reason ‚Üí Act ‚Üí Observe ‚Üí Repeat\n",
    "\n",
    "This is the **core loop** that turns a graph into an agent.\n",
    "\n",
    "### The ReAct Flow\n",
    "\n",
    "```\n",
    "    Think (Reasoner Node)\n",
    "         ‚Üì\n",
    "    Act (Tool Node)\n",
    "         ‚Üì\n",
    "    Observe (Tool Result)\n",
    "         ‚Üì\n",
    "    Repeat or Done\n",
    "```\n",
    "\n",
    "### Key Components\n",
    "\n",
    "1. **Reasoner Node** ‚Äî LLM looks at state, decides what to do next\n",
    "   - \"I need more context\" ‚Üí call a tool\n",
    "   - \"I have enough\" ‚Üí produce final answer\n",
    "\n",
    "2. **Tool Node** ‚Äî Executes the tool, returns observation\n",
    "\n",
    "3. **Conditional Router** ‚Äî Decides whether to continue or finish\n",
    "\n",
    "**The LLM doesn't just answer. It *thinks* about what it needs, *acts* to get it, *observes* the result, and *decides again*.**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 1: Define Tools\n",
    "\n",
    "Tools are functions the agent can call to get information or perform actions.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define REAL tools for our agent\n",
    "\n",
    "@tool\n",
    "def tavily_search(query: str) -> str:\n",
    "    \"\"\"Search the web for current information using Tavily API.\n",
    "    \n",
    "    This is a REAL web search tool that demonstrates the ReAct architecture.\n",
    "    The agent can use this to gather up-to-date information from the internet.\n",
    "    \n",
    "    Args:\n",
    "        query: The search query to look up on the web\n",
    "    \n",
    "    Returns:\n",
    "        A formatted summary of search results with relevant information\n",
    "    \"\"\"\n",
    "    if not tavily_client:\n",
    "        return \"Tavily API not available. Please install tavily-python (`pip install tavily-python`) and set TAVILY_API_KEY in .env file.\"\n",
    "    \n",
    "    try:\n",
    "        # Perform real web search using Tavily\n",
    "        response = tavily_client.search(\n",
    "            query=query,\n",
    "            max_results=5,\n",
    "            search_depth=\"advanced\"\n",
    "        )\n",
    "        \n",
    "        # Format the results\n",
    "        results = []\n",
    "        if response.get('results'):\n",
    "            for result in response['results']:\n",
    "                title = result.get('title', 'No title')\n",
    "                url = result.get('url', '')\n",
    "                content = result.get('content', '')\n",
    "                results.append(f\"**{title}**\\n{content[:200]}...\\nSource: {url}\")\n",
    "        \n",
    "        if results:\n",
    "            return \"\\n\\n\".join(results)\n",
    "        else:\n",
    "            return f\"No results found for query: {query}\"\n",
    "    except Exception as e:\n",
    "        return f\"Error searching: {str(e)}\"\n",
    "\n",
    "\n",
    "@tool\n",
    "def get_todays_events() -> str:\n",
    "    \"\"\"Get today's calendar events from the calendar.\n",
    "    \n",
    "    Returns a list of today's scheduled meetings and events.\n",
    "    This is a dummy calendar implementation for demonstration.\n",
    "    \n",
    "    Returns:\n",
    "        A formatted list of today's calendar events\n",
    "    \"\"\"\n",
    "    # Dummy calendar with sample events\n",
    "    today = datetime.now().strftime(\"%Y-%m-%d\")\n",
    "    events = [\n",
    "        {\n",
    "            \"time\": \"10:00 AM\",\n",
    "            \"title\": \"Team Standup\",\n",
    "            \"attendees\": [\"Team\"],\n",
    "            \"location\": \"Zoom\"\n",
    "        },\n",
    "        {\n",
    "            \"time\": \"2:00 PM\",\n",
    "            \"title\": \"Meeting with Marc Kligen about Langfuse Eval\",\n",
    "            \"attendees\": [\"Marc Kligen (Langfuse)\", \"You\"],\n",
    "            \"location\": \"Google Meet\",\n",
    "            \"topic\": \"Langfuse Eval features and integration\"\n",
    "        },\n",
    "        {\n",
    "            \"time\": \"4:30 PM\",\n",
    "            \"title\": \"Product Review\",\n",
    "            \"attendees\": [\"Product Team\"],\n",
    "            \"location\": \"Conference Room A\"\n",
    "        }\n",
    "    ]\n",
    "    \n",
    "    formatted_events = [f\"üìÖ {today} - Today's Events:\\n\"]\n",
    "    for event in events:\n",
    "        formatted_events.append(\n",
    "            f\"  ‚è∞ {event['time']}: {event['title']}\\n\"\n",
    "            f\"     üë• Attendees: {', '.join(event['attendees'])}\\n\"\n",
    "            f\"     üìç Location: {event['location']}\"\n",
    "        )\n",
    "        if 'topic' in event:\n",
    "            formatted_events.append(f\"     üí¨ Topic: {event['topic']}\")\n",
    "        formatted_events.append(\"\")\n",
    "    \n",
    "    return \"\\n\".join(formatted_events)\n",
    "\n",
    "\n",
    "@tool\n",
    "def get_current_date() -> str:\n",
    "    \"\"\"Returns today's date and time for context.\n",
    "    \n",
    "    Returns:\n",
    "        Current date and time in a readable format\n",
    "    \"\"\"\n",
    "    return datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\")\n",
    "\n",
    "\n",
    "# Create a list of tools\n",
    "tools = [tavily_search, get_todays_events, get_current_date]\n",
    "\n",
    "print(f\"‚úÖ Defined {len(tools)} tools:\")\n",
    "for t in tools:\n",
    "    print(f\"  - {t.name}\")\n",
    "    if t.name == \"tavily_search\" and tavily_client:\n",
    "        print(\"    üîç Real web search API - demonstrates ReAct architecture!\")\n",
    "    elif t.name == \"get_todays_events\":\n",
    "        print(\"    üìÖ Calendar tool - shows today's meetings\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 2: Create the LLM with Tools\n",
    "\n",
    "We'll use OpenAI's ChatOpenAI model, bound with our tools so it can call them.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the LLM with tools bound\n",
    "# The API key is loaded from .env file in the imports cell above\n",
    "\n",
    "if api_key:\n",
    "    # Create the LLM instance\n",
    "    llm = ChatOpenAI(model=\"gpt-4o-mini\", temperature=0)\n",
    "    \n",
    "    # Bind tools to the LLM so it can call them\n",
    "    llm_with_tools = llm.bind_tools(tools)\n",
    "    \n",
    "    print(\"‚úÖ LLM initialized with tools\")\n",
    "    print(f\"üìù Model: gpt-4o-mini\")\n",
    "    print(f\"üîß Tools available: {[tool.name for tool in tools]}\")\n",
    "else:\n",
    "    llm = None\n",
    "    llm_with_tools = None\n",
    "    print(\"‚ö†Ô∏è  Warning: Cannot initialize LLM without API key\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 3: Define the ReAct Nodes\n",
    "\n",
    "We need two main nodes:\n",
    "1. **Reasoner Node** ‚Äî Calls the LLM to decide what to do\n",
    "2. **Tool Node** ‚Äî Executes the tool and returns the observation\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reasoner node using actual LLM\n",
    "def reasoner_node(state: AgentState) -> dict:\n",
    "    \"\"\"Reasoner node: LLM decides what to do next.\n",
    "    \n",
    "    This is the 'Think' step of the ReAct loop.\n",
    "    The LLM examines the current state and either:\n",
    "    - Calls a tool (needs more info)\n",
    "    - Returns a final answer (has enough info)\n",
    "    \"\"\"\n",
    "    messages = state.get(\"messages\", [])\n",
    "    \n",
    "    if not messages:\n",
    "        return {\"messages\": [AIMessage(\"I need a question to help with!\")]}\n",
    "    \n",
    "    last_message = messages[-1]\n",
    "    \n",
    "    # If we already have a final answer, we're done\n",
    "    if state.get(\"final_answer\"):\n",
    "        return {}\n",
    "    \n",
    "    '''\n",
    "    # ‚îÄ‚îÄ Fallback if no LLM configured ‚îÄ‚îÄ\n",
    "    if not llm_with_tools:\n",
    "        if isinstance(last_message, HumanMessage):\n",
    "            content = last_message.content.lower()\n",
    "            if \"marc\" in content or \"langfuse\" in content or \"eval\" in content:\n",
    "                tool_call_message = AIMessage(\n",
    "                    content=\"\",\n",
    "                    tool_calls=[{\n",
    "                        \"name\": \"tavily_search\",\n",
    "                        \"args\": {\"query\": \"Langfuse Eval Marc Kligen\"},\n",
    "                        \"id\": \"call_123\"\n",
    "                    }]\n",
    "                )\n",
    "                return {\n",
    "                    \"messages\": [tool_call_message],\n",
    "                    \"steps\": [{\"type\": \"reason\", \"content\": \"I need to research Langfuse Eval and Marc Kligen.\"}]\n",
    "                }\n",
    "            else:\n",
    "                final = \"I can help you prepare. Let me gather context first.\"\n",
    "                return {\"messages\": [AIMessage(final)], \"final_answer\": final}\n",
    "        elif isinstance(last_message, ToolMessage):\n",
    "            tool_result = last_message.content\n",
    "            final = f\"Based on research:\\n{tool_result[:300]}\\n\\nMeeting prep complete!\"\n",
    "            return {\n",
    "                \"messages\": [AIMessage(final)],\n",
    "                \"steps\": [{\"type\": \"reason\", \"content\": \"Generating final meeting prep summary.\"}],\n",
    "                \"final_answer\": final\n",
    "            }\n",
    "        return {}\n",
    "    '''\n",
    "    # ‚îÄ‚îÄ Use actual LLM ‚îÄ‚îÄ\n",
    "    try:\n",
    "        if isinstance(last_message, ToolMessage):\n",
    "            # After tool execution: synthesize a final answer (no tools bound)\n",
    "            # Build clean message history to avoid tool_call_id mismatch errors\n",
    "            clean_messages = []\n",
    "            tool_msgs = []\n",
    "            ai_with_tools = None\n",
    "            human_msg = None\n",
    "            \n",
    "            for msg in reversed(messages):\n",
    "                if isinstance(msg, ToolMessage):\n",
    "                    tool_msgs.insert(0, msg)\n",
    "                elif isinstance(msg, AIMessage) and getattr(msg, 'tool_calls', None):\n",
    "                    ai_with_tools = msg\n",
    "                    break\n",
    "                elif isinstance(msg, HumanMessage) and human_msg is None:\n",
    "                    human_msg = msg\n",
    "            \n",
    "            if human_msg and ai_with_tools and tool_msgs:\n",
    "                clean_messages = [human_msg, ai_with_tools] + tool_msgs\n",
    "            else:\n",
    "                clean_messages = messages[-3:] if len(messages) >= 3 else messages\n",
    "            \n",
    "            try:\n",
    "                response = llm.invoke(clean_messages)          # plain llm, no tools\n",
    "                final_answer = response.content\n",
    "                return {\n",
    "                    \"messages\": [response],\n",
    "                    \"steps\": [{\"type\": \"reason\", \"content\": \"Synthesizing final answer from tool results.\"}],\n",
    "                    \"final_answer\": final_answer\n",
    "                }\n",
    "            except Exception as e:\n",
    "                print(f\"‚ö†Ô∏è  Error calling LLM: {e}\")\n",
    "                final_answer = f\"Based on tool results: {last_message.content}\"\n",
    "                return {\n",
    "                    \"messages\": [AIMessage(final_answer)],\n",
    "                    \"steps\": [{\"type\": \"reason\", \"content\": \"Generated answer from tool results.\"}],\n",
    "                    \"final_answer\": final_answer\n",
    "                }\n",
    "        else:\n",
    "            # First reasoning step: ask LLM (with tools) what to do\n",
    "            response = llm_with_tools.invoke(messages)\n",
    "            \n",
    "            if hasattr(response, 'tool_calls') and response.tool_calls:\n",
    "                return {\n",
    "                    \"messages\": [response],\n",
    "                    \"steps\": [{\"type\": \"reason\",\n",
    "                               \"content\": response.content or \"Deciding to call a tool to gather more information.\"}]\n",
    "                }\n",
    "            else:\n",
    "                final_answer = response.content\n",
    "                return {\n",
    "                    \"messages\": [response],\n",
    "                    \"steps\": [{\"type\": \"reason\", \"content\": \"Generated final answer.\"}],\n",
    "                    \"final_answer\": final_answer\n",
    "                }\n",
    "    except Exception as e:\n",
    "        print(f\"‚ö†Ô∏è  Error calling LLM: {e}\")\n",
    "        return {\n",
    "            \"messages\": [AIMessage(f\"Error: {str(e)}\")],\n",
    "            \"final_answer\": f\"Error: {str(e)}\"\n",
    "        }\n",
    "\n",
    "\n",
    "def tool_node(state: AgentState) -> dict:\n",
    "    \"\"\"Tool node: Executes tool calls and returns observations.\n",
    "    \n",
    "    This is the 'Act' step of the ReAct loop.\n",
    "    It finds the last AIMessage with tool_calls, executes each,\n",
    "    and returns ToolMessage results.\n",
    "    \"\"\"\n",
    "    messages = state.get(\"messages\", [])\n",
    "    last_message = messages[-1] if messages else None\n",
    "    \n",
    "    if not last_message or not getattr(last_message, 'tool_calls', None):\n",
    "        return {}\n",
    "    \n",
    "    tool_map = {t.name: t for t in tools}\n",
    "    tool_messages = []\n",
    "    tool_steps = []\n",
    "    \n",
    "    for tc in last_message.tool_calls:\n",
    "        name = tc[\"name\"]\n",
    "        args = tc[\"args\"]\n",
    "        call_id = tc[\"id\"]\n",
    "        \n",
    "        if name in tool_map:\n",
    "            try:\n",
    "                result = tool_map[name].invoke(args)\n",
    "                tool_messages.append(ToolMessage(content=str(result), tool_call_id=call_id))\n",
    "                tool_steps.append({\"type\": \"act\", \"tool\": name, \"args\": args, \"result\": str(result)[:200]})\n",
    "            except Exception as e:\n",
    "                tool_messages.append(ToolMessage(content=f\"Error: {e}\", tool_call_id=call_id))\n",
    "        else:\n",
    "            tool_messages.append(ToolMessage(content=f\"Unknown tool: {name}\", tool_call_id=call_id))\n",
    "    \n",
    "    return {\"messages\": tool_messages, \"steps\": tool_steps} if tool_messages else {}\n",
    "\n",
    "\n",
    "print(\"‚úÖ ReAct nodes defined: reasoner_node, tool_node\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# üéØ Concept 3: Conditional Edges & Termination\n",
    "\n",
    "## The Router: How Your Graph Decides What Happens Next\n",
    "\n",
    "**Agents are just graphs with loops + a stopping policy.**\n",
    "\n",
    "### The Routing Function\n",
    "\n",
    "The router examines the state and decides:\n",
    "- If LLM requested a tool ‚Üí route to `tool_node`\n",
    "- If LLM said DONE ‚Üí route to `final_node`\n",
    "- If parsing failed ‚Üí route to `retry_node` (optional)\n",
    "\n",
    "### Critical: Add a Max-Steps Guard\n",
    "\n",
    "**Always** add a maximum steps limit to prevent infinite loops!\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Router: decides whether to continue or finish\n",
    "MAX_STEPS = 10  # Safety guard against infinite loops\n",
    "\n",
    "def should_continue(state: AgentState) -> Literal[\"tool_node\", \"__end__\"]:\n",
    "    \"\"\"Conditional edge: route to tool_node or END.\n",
    "    \n",
    "    Checks:\n",
    "    1. If we have a final answer ‚Üí END\n",
    "    2. If the last AI message has tool_calls ‚Üí tool_node\n",
    "    3. If we've exceeded MAX_STEPS ‚Üí END (safety)\n",
    "    4. Otherwise ‚Üí END\n",
    "    \"\"\"\n",
    "    # Safety: max steps guard\n",
    "    if len(state.get(\"steps\", [])) >= MAX_STEPS:\n",
    "        print(f\"‚ö†Ô∏è  Max steps ({MAX_STEPS}) reached ‚Äî terminating\")\n",
    "        return \"__end__\"\n",
    "    \n",
    "    # If final answer is set, we're done\n",
    "    if state.get(\"final_answer\"):\n",
    "        return \"__end__\"\n",
    "    \n",
    "    # Check last message\n",
    "    messages = state.get(\"messages\", [])\n",
    "    if messages:\n",
    "        last = messages[-1]\n",
    "        if isinstance(last, AIMessage) and getattr(last, 'tool_calls', None):\n",
    "            return \"tool_node\"\n",
    "    \n",
    "    return \"__end__\"\n",
    "\n",
    "\n",
    "print(\"‚úÖ Router defined: should_continue()\")\n",
    "print(f\"   Max steps guard: {MAX_STEPS}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Building the ReAct Graph\n",
    "\n",
    "Now let's wire everything together:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build the ReAct graph\n",
    "workflow = StateGraph(AgentState)\n",
    "\n",
    "# Add nodes\n",
    "workflow.add_node(\"reasoner\", reasoner_node)\n",
    "workflow.add_node(\"tool_node\", tool_node)\n",
    "\n",
    "# Add edges\n",
    "workflow.add_edge(START, \"reasoner\")                     # START ‚Üí reasoner\n",
    "workflow.add_conditional_edges(\"reasoner\", should_continue)  # reasoner ‚Üí tool_node | END\n",
    "workflow.add_edge(\"tool_node\", \"reasoner\")               # tool_node ‚Üí reasoner (loop back)\n",
    "\n",
    "# Compile the graph (no checkpointer yet)\n",
    "react_app = workflow.compile()\n",
    "\n",
    "print(\"‚úÖ ReAct graph compiled!\")\n",
    "print(\"   START ‚Üí reasoner ‚îÄ‚î¨‚îÄ‚Üí tool_node ‚Üí reasoner (loop)\")\n",
    "print(\"                     ‚îî‚îÄ‚Üí END\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualize the ReAct Graph\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize the ReAct graph\n",
    "try:\n",
    "    from IPython.display import Image, display\n",
    "    display(Image(react_app.get_graph().draw_mermaid_png()))\n",
    "except Exception as e:\n",
    "    # Fallback: print the mermaid diagram\n",
    "    print(\"Graph structure (Mermaid):\")\n",
    "    print(react_app.get_graph().draw_mermaid())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test the ReAct Agent\n",
    "\n",
    "Let's test our agent with a simple query:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test the ReAct agent with a simple query\n",
    "print(\"=\" * 60)\n",
    "print(\"REACT AGENT TEST\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "initial_state = {\n",
    "    \"messages\": [HumanMessage(\"What is the current date and time?\")],\n",
    "    \"steps\": [],\n",
    "    \"final_answer\": \"\"\n",
    "}\n",
    "\n",
    "result = react_app.invoke(initial_state)\n",
    "\n",
    "# Display the trace\n",
    "print(\"\\nüìã REACT TRACE:\")\n",
    "for i, step in enumerate(result.get(\"steps\", []), 1):\n",
    "    step_type = step.get(\"type\", \"unknown\")\n",
    "    if step_type == \"reason\":\n",
    "        print(f\"  Step {i} - üí≠ REASON: {step.get('content', '')[:100]}\")\n",
    "    elif step_type == \"act\":\n",
    "        print(f\"  Step {i} - üîß ACT: {step.get('tool', '')}({step.get('args', {})})\")\n",
    "        print(f\"           üìä Result: {step.get('result', '')[:100]}\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"FINAL ANSWER:\")\n",
    "print(\"=\" * 60)\n",
    "print(result.get(\"final_answer\", \"No answer generated\"))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## üéØ ReAct Examples: Real-World Patterns\n",
    "\n",
    "Now let's see the ReAct pattern in action with **real tools**! These examples demonstrate different agent behaviors:\n",
    "\n",
    "1. **Example 1: Simple Single Tool Call** - Agent uses one tool to answer a question\n",
    "2. **Example 2: Multi-Step Reasoning** - Agent chains multiple tool calls to solve a complex task\n",
    "3. **Example 3: Tool Selection** - Agent chooses the right tool from multiple options\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Example 1: Simple Single Tool Call\n",
    "\n",
    "**Pattern:** User asks a question ‚Üí Agent reasons ‚Üí Agent calls ONE tool ‚Üí Agent synthesizes answer\n",
    "\n",
    "This is the simplest ReAct pattern. The agent makes a single tool call to gather information, then provides a final answer.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example 1: Single Tool Call\n",
    "# The agent searches the web to answer a factual question\n",
    "print(\"=\" * 60)\n",
    "print(\"EXAMPLE 1: Simple Single Tool Call\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "state_ex1 = {\n",
    "    \"messages\": [HumanMessage(\"What is Langfuse and what does it do?\")],\n",
    "    \"steps\": [],\n",
    "    \"final_answer\": \"\"\n",
    "}\n",
    "\n",
    "result_ex1 = react_app.invoke(state_ex1)\n",
    "\n",
    "# Show the ReAct trace\n",
    "print(\"\\nüìã REACT TRACE:\")\n",
    "for i, step in enumerate(result_ex1.get(\"steps\", []), 1):\n",
    "    step_type = step.get(\"type\", \"unknown\")\n",
    "    if step_type == \"reason\":\n",
    "        print(f\"\\n  Step {i} - üí≠ REASON:\")\n",
    "        print(f\"    {step.get('content', '')[:150]}\")\n",
    "    elif step_type == \"act\":\n",
    "        print(f\"\\n  Step {i} - üîß ACT: {step.get('tool', '')}()\")\n",
    "        result_preview = step.get('result', '')[:150]\n",
    "        print(f\"    üìä Result: {result_preview}...\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"‚úÖ FINAL ANSWER:\")\n",
    "print(\"=\" * 60)\n",
    "print(result_ex1.get(\"final_answer\", \"No answer\")[:500])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Example 2: Multi-Step Reasoning\n",
    "\n",
    "**Pattern:** User asks a complex question ‚Üí Agent reasons ‚Üí Agent calls MULTIPLE tools in sequence ‚Üí Agent synthesizes comprehensive answer\n",
    "\n",
    "This demonstrates how agents can chain tool calls. The agent might first check the calendar, then search for additional context, then provide a comprehensive answer.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example 2: Multi-Step Reasoning\n",
    "# The agent checks the calendar FIRST, then searches for context\n",
    "print(\"=\" * 60)\n",
    "print(\"EXAMPLE 2: Multi-Step Reasoning\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "state_ex2 = {\n",
    "    \"messages\": [HumanMessage(\n",
    "        \"Check my calendar for today and then research the topics \"\n",
    "        \"for any meetings I have so I can be prepared.\"\n",
    "    )],\n",
    "    \"steps\": [],\n",
    "    \"final_answer\": \"\"\n",
    "}\n",
    "\n",
    "result_ex2 = react_app.invoke(state_ex2)\n",
    "\n",
    "# Show the ReAct trace\n",
    "print(\"\\nüìã REACT TRACE:\")\n",
    "for i, step in enumerate(result_ex2.get(\"steps\", []), 1):\n",
    "    step_type = step.get(\"type\", \"unknown\")\n",
    "    if step_type == \"reason\":\n",
    "        print(f\"\\n  Step {i} - üí≠ REASON:\")\n",
    "        print(f\"    {step.get('content', '')[:150]}\")\n",
    "    elif step_type == \"act\":\n",
    "        print(f\"\\n  Step {i} - üîß ACT: {step.get('tool', '')}()\")\n",
    "        result_preview = step.get('result', '')[:150]\n",
    "        print(f\"    üìä Result: {result_preview}...\")\n",
    "\n",
    "print(f\"\\nüìä Total steps: {len(result_ex2.get('steps', []))}\")\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"‚úÖ FINAL ANSWER:\")\n",
    "print(\"=\" * 60)\n",
    "print(result_ex2.get(\"final_answer\", \"No answer\")[:800])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Example 3: Tool Selection\n",
    "\n",
    "**Pattern:** User asks a question ‚Üí Agent reasons ‚Üí Agent SELECTS the most appropriate tool ‚Üí Agent provides answer\n",
    "\n",
    "This shows how agents intelligently choose between available tools based on the query. The agent must decide whether to search the web, check the calendar, or get the current date.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example 3: Tool Selection\n",
    "# The agent must CHOOSE the right tool ‚Äî no web search needed here\n",
    "print(\"=\" * 60)\n",
    "print(\"EXAMPLE 3: Tool Selection\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "state_ex3 = {\n",
    "    \"messages\": [HumanMessage(\"What meetings do I have today and what time is it now?\")],\n",
    "    \"steps\": [],\n",
    "    \"final_answer\": \"\"\n",
    "}\n",
    "\n",
    "result_ex3 = react_app.invoke(state_ex3)\n",
    "\n",
    "# Show the ReAct trace\n",
    "print(\"\\nüìã REACT TRACE:\")\n",
    "for i, step in enumerate(result_ex3.get(\"steps\", []), 1):\n",
    "    step_type = step.get(\"type\", \"unknown\")\n",
    "    if step_type == \"reason\":\n",
    "        print(f\"\\n  Step {i} - üí≠ REASON:\")\n",
    "        print(f\"    {step.get('content', '')[:150]}\")\n",
    "    elif step_type == \"act\":\n",
    "        print(f\"\\n  Step {i} - üîß ACT: {step.get('tool', '')}()\")\n",
    "        result_preview = step.get('result', '')[:150]\n",
    "        print(f\"    üìä Result: {result_preview}...\")\n",
    "\n",
    "print(f\"\\nüìä Total steps: {len(result_ex3.get('steps', []))}\")\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"‚úÖ FINAL ANSWER:\")\n",
    "print(\"=\" * 60)\n",
    "print(result_ex3.get(\"final_answer\", \"No answer\")[:500])\n",
    "\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"üéØ KEY TAKEAWAY:\")\n",
    "print(\"The agent chose get_todays_events AND get_current_date\")\n",
    "print(\"without using tavily_search ‚Äî because web search wasn't needed!\")\n",
    "print(\"=\" * 60)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# üíæ Concept 4: Checkpointing\n",
    "\n",
    "## Your Agent Gets Memory and Replay\n",
    "\n",
    "**Checkpointing** saves the full state after every node execution. This enables:\n",
    "\n",
    "| Feature | Description |\n",
    "|---------|-------------|\n",
    "| **Memory** | Same `thread_id` = agent remembers the entire conversation |\n",
    "| **Fault Tolerance** | Crash mid-run? Resume from last checkpoint |\n",
    "| **Time Travel** | Replay any past state, branch from any point |\n",
    "| **Debugging** | Inspect exactly what the agent saw at each step |\n",
    "\n",
    "### Without Checkpointing\n",
    "\n",
    "Every invocation starts from scratch. No memory, no recovery.\n",
    "\n",
    "### With Checkpointing\n",
    "\n",
    "Full state saved after every node execution. Persistent across runs.\n",
    "\n",
    "**Reference:** [LangGraph Official Documentation - Persistence](https://docs.langchain.com/oss/python/langgraph/overview)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Setting Up Checkpointing\n",
    "\n",
    "We'll use **MemorySaver** as our checkpoint store for in-memory persistence. This is the standard approach recommended in the [official LangGraph documentation](https://docs.langchain.com/oss/python/langgraph/overview).\n",
    "\n",
    "**Note:** For production deployments, you can use other checkpoint stores (like Postgres, SQLite, or custom implementations), but MemorySaver is perfect for learning and development.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up checkpointing with MemorySaver\n",
    "memory = MemorySaver()\n",
    "\n",
    "# Recompile the graph WITH checkpointing enabled\n",
    "react_app_with_memory = workflow.compile(checkpointer=memory)\n",
    "\n",
    "print(\"‚úÖ Graph recompiled with MemorySaver checkpointing!\")\n",
    "print(\"   Now the agent remembers conversations across invocations.\")\n",
    "print(\"   Use 'thread_id' in config to maintain conversation state.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Using Thread IDs for Memory\n",
    "\n",
    "**Thread ID = Conversation Identity**\n",
    "\n",
    "- Same `thread_id` ‚Üí Agent remembers everything\n",
    "- Different `thread_id` ‚Üí Fresh start, clean state\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Demonstrate memory with thread IDs\n",
    "print(\"=\" * 60)\n",
    "print(\"CHECKPOINTING DEMO: Memory with Thread IDs\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# Thread 1: First message\n",
    "config_thread1 = {\"configurable\": {\"thread_id\": \"demo-thread-1\"}}\n",
    "\n",
    "state1 = {\n",
    "    \"messages\": [HumanMessage(\"Hi! My name is Alex and I'm working on a LangGraph project.\")],\n",
    "    \"steps\": [],\n",
    "    \"final_answer\": \"\"\n",
    "}\n",
    "\n",
    "result1 = react_app_with_memory.invoke(state1, config=config_thread1)\n",
    "print(\"\\nüßµ Thread 1 ‚Äî Message 1:\")\n",
    "print(f\"   User: {state1['messages'][0].content}\")\n",
    "print(f\"   Agent: {result1.get('final_answer', '')[:200]}\")\n",
    "\n",
    "# Thread 1: Second message ‚Äî agent should REMEMBER the name\n",
    "state2 = {\n",
    "    \"messages\": [HumanMessage(\"What's my name and what am I working on?\")],\n",
    "    \"steps\": [],\n",
    "    \"final_answer\": \"\"\n",
    "}\n",
    "\n",
    "result2 = react_app_with_memory.invoke(state2, config=config_thread1)\n",
    "print(f\"\\nüßµ Thread 1 ‚Äî Message 2 (same thread ‚Äî should remember!):\")\n",
    "print(f\"   User: {state2['messages'][0].content}\")\n",
    "print(f\"   Agent: {result2.get('final_answer', '')[:200]}\")\n",
    "\n",
    "# Thread 2: Different thread ‚Äî agent should NOT remember\n",
    "config_thread2 = {\"configurable\": {\"thread_id\": \"demo-thread-2\"}}\n",
    "\n",
    "state3 = {\n",
    "    \"messages\": [HumanMessage(\"What's my name?\")],\n",
    "    \"steps\": [],\n",
    "    \"final_answer\": \"\"\n",
    "}\n",
    "\n",
    "result3 = react_app_with_memory.invoke(state3, config=config_thread2)\n",
    "print(f\"\\nüßµ Thread 2 ‚Äî Different thread (should NOT remember!):\")\n",
    "print(f\"   User: {state3['messages'][0].content}\")\n",
    "print(f\"   Agent: {result3.get('final_answer', '')[:200]}\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"üéØ KEY TAKEAWAY:\")\n",
    "print(\"   Same thread_id ‚Üí Agent remembers everything\")\n",
    "print(\"   Different thread_id ‚Üí Fresh start, no memory\")\n",
    "print(\"=\" * 60)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# üèóÔ∏è Live Build: Meeting Prep Agent\n",
    "\n",
    "## Complete Implementation\n",
    "\n",
    "Let's build a complete Meeting Prep Agent that:\n",
    "- Takes a meeting request\n",
    "- Uses tools to gather context\n",
    "- Generates agenda bullets, questions, and demo ideas\n",
    "\n",
    "**Prompt:** Prep me for a meeting with Marc Kligen about Langfuse Eval. Give me 5 agenda bullets, 3 questions to ask, and one demo idea.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Meeting Prep Agent ‚Äî uses the same ReAct graph but with a rich prompt\n",
    "# This demonstrates a real-world use case\n",
    "\n",
    "class MeetingState(TypedDict):\n",
    "    messages: Annotated[list, operator.add]\n",
    "    steps: Annotated[list, operator.add]\n",
    "    final_answer: str\n",
    "\n",
    "def meeting_reasoner(state: MeetingState) -> dict:\n",
    "    \"\"\"Meeting prep reasoner: same pattern, richer prompt.\"\"\"\n",
    "    messages = state.get(\"messages\", [])\n",
    "    if not messages:\n",
    "        return {}\n",
    "    \n",
    "    last_message = messages[-1]\n",
    "    \n",
    "    if state.get(\"final_answer\"):\n",
    "        return {}\n",
    "    \n",
    "    if not llm_with_tools:\n",
    "        # Fallback\n",
    "        if isinstance(last_message, ToolMessage):\n",
    "            final = f\"Meeting prep based on research:\\n{last_message.content[:300]}\"\n",
    "            return {\"messages\": [AIMessage(final)], \"final_answer\": final,\n",
    "                    \"steps\": [{\"type\": \"reason\", \"content\": \"Generated meeting prep.\"}]}\n",
    "        return {\n",
    "            \"messages\": [AIMessage(content=\"\", tool_calls=[{\n",
    "                \"name\": \"tavily_search\", \"args\": {\"query\": \"Langfuse Eval\"}, \"id\": \"call_meet_1\"\n",
    "            }])],\n",
    "            \"steps\": [{\"type\": \"reason\", \"content\": \"Searching for meeting context.\"}]\n",
    "        }\n",
    "    \n",
    "    try:\n",
    "        if isinstance(last_message, ToolMessage):\n",
    "            clean = []\n",
    "            tool_msgs, ai_msg, human_msg = [], None, None\n",
    "            for msg in reversed(messages):\n",
    "                if isinstance(msg, ToolMessage):\n",
    "                    tool_msgs.insert(0, msg)\n",
    "                elif isinstance(msg, AIMessage) and getattr(msg, 'tool_calls', None):\n",
    "                    ai_msg = msg\n",
    "                    break\n",
    "                elif isinstance(msg, HumanMessage) and not human_msg:\n",
    "                    human_msg = msg\n",
    "            clean = ([human_msg] if human_msg else []) + ([ai_msg] if ai_msg else []) + tool_msgs\n",
    "            if not clean:\n",
    "                clean = messages[-3:]\n",
    "            \n",
    "            response = llm.invoke(clean)\n",
    "            return {\n",
    "                \"messages\": [response],\n",
    "                \"steps\": [{\"type\": \"reason\", \"content\": \"Synthesizing meeting prep.\"}],\n",
    "                \"final_answer\": response.content\n",
    "            }\n",
    "        else:\n",
    "            response = llm_with_tools.invoke(messages)\n",
    "            if getattr(response, 'tool_calls', None):\n",
    "                return {\n",
    "                    \"messages\": [response],\n",
    "                    \"steps\": [{\"type\": \"reason\",\n",
    "                               \"content\": response.content or \"Calling tools for meeting context.\"}]\n",
    "                }\n",
    "            return {\n",
    "                \"messages\": [response],\n",
    "                \"steps\": [{\"type\": \"reason\", \"content\": \"Generated meeting prep.\"}],\n",
    "                \"final_answer\": response.content\n",
    "            }\n",
    "    except Exception as e:\n",
    "        err = f\"Error: {e}\"\n",
    "        return {\"messages\": [AIMessage(err)], \"final_answer\": err}\n",
    "\n",
    "def meeting_tool_node(state: MeetingState) -> dict:\n",
    "    \"\"\"Execute tools for meeting prep.\"\"\"\n",
    "    msgs = state.get(\"messages\", [])\n",
    "    last = msgs[-1] if msgs else None\n",
    "    if not last or not getattr(last, 'tool_calls', None):\n",
    "        return {}\n",
    "    \n",
    "    tool_map = {t.name: t for t in tools}\n",
    "    tool_messages, tool_steps = [], []\n",
    "    for tc in last.tool_calls:\n",
    "        name, args, cid = tc[\"name\"], tc[\"args\"], tc[\"id\"]\n",
    "        if name in tool_map:\n",
    "            try:\n",
    "                result = tool_map[name].invoke(args)\n",
    "                tool_messages.append(ToolMessage(content=str(result), tool_call_id=cid))\n",
    "                tool_steps.append({\"type\": \"act\", \"tool\": name, \"args\": args, \"result\": str(result)[:200]})\n",
    "            except Exception as e:\n",
    "                tool_messages.append(ToolMessage(content=f\"Error: {e}\", tool_call_id=cid))\n",
    "        else:\n",
    "            tool_messages.append(ToolMessage(content=f\"Unknown tool: {name}\", tool_call_id=cid))\n",
    "    return {\"messages\": tool_messages, \"steps\": tool_steps} if tool_messages else {}\n",
    "\n",
    "def meeting_router(state: MeetingState) -> Literal[\"meeting_tools\", \"__end__\"]:\n",
    "    if len(state.get(\"steps\", [])) >= MAX_STEPS:\n",
    "        return \"__end__\"\n",
    "    if state.get(\"final_answer\"):\n",
    "        return \"__end__\"\n",
    "    msgs = state.get(\"messages\", [])\n",
    "    if msgs and isinstance(msgs[-1], AIMessage) and getattr(msgs[-1], 'tool_calls', None):\n",
    "        return \"meeting_tools\"\n",
    "    return \"__end__\"\n",
    "\n",
    "# Build meeting prep graph\n",
    "meeting_wf = StateGraph(MeetingState)\n",
    "meeting_wf.add_node(\"meeting_reasoner\", meeting_reasoner)\n",
    "meeting_wf.add_node(\"meeting_tools\", meeting_tool_node)\n",
    "meeting_wf.add_edge(START, \"meeting_reasoner\")\n",
    "meeting_wf.add_conditional_edges(\"meeting_reasoner\", meeting_router)\n",
    "meeting_wf.add_edge(\"meeting_tools\", \"meeting_reasoner\")\n",
    "\n",
    "meeting_memory = MemorySaver()\n",
    "meeting_app = meeting_wf.compile(checkpointer=meeting_memory)\n",
    "\n",
    "print(\"‚úÖ Meeting Prep Agent compiled!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run the Meeting Prep Agent\n",
    "print(\"=\" * 60)\n",
    "print(\"üèóÔ∏è MEETING PREP AGENT\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "meeting_config = {\"configurable\": {\"thread_id\": \"meeting-prep-1\"}}\n",
    "\n",
    "meeting_state = {\n",
    "    \"messages\": [HumanMessage(\n",
    "        \"Prep me for a meeting with Marc Kligen about Langfuse Eval. \"\n",
    "        \"Marc Kligen is from Langfuse. \"\n",
    "        \"Give me 5 agenda bullets, 3 questions to ask, and one demo idea.\"\n",
    "    )],\n",
    "    \"steps\": [],\n",
    "    \"final_answer\": \"\"\n",
    "}\n",
    "\n",
    "result_meeting = meeting_app.invoke(meeting_state, config=meeting_config)\n",
    "\n",
    "# Display the trace\n",
    "print(\"\\nüìã REACT TRACE:\")\n",
    "for i, step in enumerate(result_meeting.get(\"steps\", []), 1):\n",
    "    step_type = step.get(\"type\", \"unknown\")\n",
    "    if step_type == \"reason\":\n",
    "        print(f\"\\n  Step {i} - üí≠ REASON:\")\n",
    "        print(f\"    {step.get('content', '')[:150]}\")\n",
    "    elif step_type == \"act\":\n",
    "        print(f\"\\n  Step {i} - üîß ACT: {step.get('tool', '')}()\")\n",
    "        print(f\"    üìä Result: {step.get('result', '')[:150]}...\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"‚úÖ MEETING PREP OUTPUT:\")\n",
    "print(\"=\" * 60)\n",
    "print(result_meeting.get(\"final_answer\", \"No answer generated\"))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# üé® Interactive Streamlit UI\n",
    "\n",
    "Let's build a beautiful Streamlit interface to interact with our ReAct agent! This UI demonstrates the agent in action with real-time responses.\n",
    "\n",
    "**Features:**\n",
    "- Interactive chat interface\n",
    "- Real-time ReAct trace visualization\n",
    "- Tool call visualization\n",
    "- Memory management with thread IDs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check if the Streamlit app file exists\n",
    "import os\n",
    "\n",
    "streamlit_path = os.path.join(os.path.dirname(os.path.abspath(\".\")), \n",
    "                               \"multi-agent-systems\", \"week-2\", \"streamlit_app.py\")\n",
    "\n",
    "# Check in current directory first\n",
    "if os.path.exists(\"streamlit_app.py\"):\n",
    "    streamlit_path = \"streamlit_app.py\"\n",
    "\n",
    "if os.path.exists(streamlit_path):\n",
    "    print(\"‚úÖ Streamlit app file found: streamlit_app.py\")\n",
    "    print(\"\\nüìù To run the Streamlit UI:\")\n",
    "    print(\"   1. Make sure all dependencies are installed:\")\n",
    "    print(\"      pip install -r requirements.txt\")\n",
    "    print(\"   2. Ensure your .env file has:\")\n",
    "    print(\"      - OPENAI_API_KEY=your_key\")\n",
    "    print(\"      - TAVILY_API_KEY=your_key (optional but recommended)\")\n",
    "    print(\"   3. Run the Streamlit app:\")\n",
    "    print(\"      streamlit run streamlit_app.py\")\n",
    "    print(\"\\nüí° The UI will open in your browser at http://localhost:8501\")\n",
    "    print(\"\\nüé® Features:\")\n",
    "    print(\"   - Interactive chat interface\")\n",
    "    print(\"   - Real-time ReAct trace visualization\")\n",
    "    print(\"   - Tool call visualization\")\n",
    "    print(\"   - Memory management with thread IDs\")\n",
    "    print(\"   - Example queries in sidebar\")\n",
    "else:\n",
    "    print(\"‚ö†Ô∏è  streamlit_app.py not found in the current directory.\")\n",
    "    print(\"   It should be in the same directory as this notebook.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Running the Streamlit App\n",
    "\n",
    "To run the interactive UI:\n",
    "\n",
    "```bash\n",
    "# Navigate to the week-2 directory\n",
    "cd AI-Internship/multi-agent-systems/week-2\n",
    "\n",
    "# Activate virtual environment (recommended)\n",
    "source venv/bin/activate\n",
    "\n",
    "# Install dependencies\n",
    "pip install -r requirements.txt\n",
    "\n",
    "# Run the app\n",
    "streamlit run streamlit_app.py\n",
    "```\n",
    "\n",
    "The app will open at **http://localhost:8501** with:\n",
    "- üí¨ **Chat interface** for interacting with the ReAct agent\n",
    "- üîç **ReAct trace** showing each Reason ‚Üí Act ‚Üí Observe step  \n",
    "- üîß **Tool visualization** showing which tools were called\n",
    "- üßµ **Thread management** for testing memory/checkpointing\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preview the Streamlit app code\n",
    "if os.path.exists(\"streamlit_app.py\"):\n",
    "    with open(\"streamlit_app.py\", \"r\") as f:\n",
    "        content = f.read()\n",
    "    print(f\"üìÑ streamlit_app.py ({len(content.splitlines())} lines)\")\n",
    "    print(\"‚îÄ\" * 40)\n",
    "    # Show first 30 lines as preview\n",
    "    for line in content.splitlines()[:30]:\n",
    "        print(line)\n",
    "    print(\"...\")\n",
    "    print(f\"‚îÄ\" * 40)\n",
    "    print(f\"(Showing first 30 of {len(content.splitlines())} lines)\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# üìù Summary\n",
    "\n",
    "Congratulations! You've learned how to build stateful, intelligent agents with LangGraph.\n",
    "\n",
    "## Key Takeaways\n",
    "\n",
    "| Concept | What You Learned |\n",
    "|---------|------------------|\n",
    "| **Typed State** | Design state with TypedDict and reducers for controlled updates |\n",
    "| **Reducers** | Use `operator.add` for append-only fields, no reducer for overwrite fields |\n",
    "| **ReAct Pattern** | Reason ‚Üí Act ‚Üí Observe ‚Üí Repeat loop that makes agents intelligent |\n",
    "| **Conditional Edges** | Router functions that decide what happens next based on state |\n",
    "| **Checkpointing** | Give agents memory using thread_id and checkpoint stores |\n",
    "| **Termination** | Always add max-steps guards to prevent infinite loops |\n",
    "\n",
    "## Patterns Covered\n",
    "\n",
    "1. ‚úÖ **Typed State Design** ‚Äî Proper state schema with reducers\n",
    "2. ‚úÖ **ReAct Agent** ‚Äî Full implementation with tools and routing\n",
    "3. ‚úÖ **Checkpointing** ‚Äî Memory and persistence across runs\n",
    "4. ‚úÖ **Meeting Prep Agent** ‚Äî Complete working example\n",
    "\n",
    "## What's Next?\n",
    "\n",
    "- **Complete the Challenge** ‚Äî Extend the Meeting Prep Agent\n",
    "- **Week 3**: Multi-Agent Systems ‚Äî Subgraphs, supervisor pattern, shared vs scoped state\n",
    "- **Week 4**: Production + Capstone ‚Äî Deploy your multi-agent system\n",
    "\n",
    "---\n",
    "\n",
    "### üîó Resources\n",
    "\n",
    "- [LangGraph Official Documentation](https://docs.langchain.com/oss/python/langgraph/overview) ‚Äî Main overview and API reference\n",
    "- [LangGraph Graph API](https://langchain-ai.github.io/langgraph/) ‚Äî Detailed graph construction guide\n",
    "- [LangGraph Persistence](https://langchain-ai.github.io/langgraph/how-tos/persistence/) ‚Äî Checkpointing and memory guide\n",
    "- [ReAct Paper](https://arxiv.org/abs/2210.03629) ‚Äî Original ReAct research paper\n",
    "- [LangSmith](https://smith.langchain.com) ‚Äî Debug & trace your graphs\n",
    "\n",
    "**Note:** This notebook uses the official LangGraph API. For checkpointing, we use `MemorySaver` which is the standard in-memory checkpoint store.\n",
    "\n",
    "---\n",
    "\n",
    "**Your Week 1 router + your Week 2 ReAct agent combine into a real multi-agent system in Week 3!** üöÄ\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# üéâ End of Week 2 Notebook - Happy Learning!\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
