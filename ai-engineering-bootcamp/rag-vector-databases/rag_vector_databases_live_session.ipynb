{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# üîç RAG + Vector Databases\n",
    "\n",
    "## Build Smarter AI Apps with LangChain\n",
    "\n",
    "Welcome to the live session on Retrieval Augmented Generation (RAG) and Vector Databases! In this notebook, you'll learn how to build production-ready RAG systems that ground LLM responses in your own data.\n",
    "\n",
    "---\n",
    "\n",
    "## üöÄ Getting Started\n",
    "\n",
    "### Prerequisites\n",
    "- Python 3.8 or higher\n",
    "- Jupyter Notebook installed\n",
    "- OpenAI API key (for embeddings and LLM calls)\n",
    "\n",
    "### Setup Instructions\n",
    "\n",
    "**Option 1: Using requirements.txt (Recommended)**\n",
    "```bash\n",
    "pip install -r requirements.txt\n",
    "```\n",
    "\n",
    "**Option 2: Manual Installation**\n",
    "```bash\n",
    "pip install langchain langchain-openai langchain-community langchain-text-splitters chromadb pypdf python-dotenv\n",
    "```\n",
    "\n",
    "### Environment Setup\n",
    "\n",
    "Create a `.env` file in this directory with your OpenAI API key:\n",
    "```\n",
    "OPENAI_API_KEY=your_api_key_here\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## üìö What You'll Learn\n",
    "\n",
    "1. **Why RAG Exists** ‚Äî Understanding LLM limitations and how RAG solves them\n",
    "2. **RAG Architecture** ‚Äî The complete flow from indexing to query\n",
    "3. **Embeddings Deep Dive** ‚Äî How text becomes vectors and why it matters\n",
    "4. **Chunking Strategies** ‚Äî The make-or-break step most people get wrong\n",
    "5. **Vector Databases** ‚Äî Storing and searching semantic data at scale\n",
    "6. **Live Build** ‚Äî Build a working Document Q&A system\n",
    "7. **Evaluation** ‚Äî How to measure and improve your RAG system\n",
    "\n",
    "### üéØ By the End of This Notebook\n",
    "\n",
    "- ‚úÖ Understand the complete RAG architecture\n",
    "- ‚úÖ Master embeddings and vector similarity search\n",
    "- ‚úÖ Build a production-ready Document Q&A system\n",
    "- ‚úÖ Know how to evaluate and debug RAG systems\n",
    "- ‚úÖ Be ready to build RAG applications on your own data\n",
    "\n",
    "---\n",
    "\n",
    "## üîë Key Concepts\n",
    "\n",
    "| Concept | What It Is | Why It Matters |\n",
    "|---------|------------|----------------|\n",
    "| **RAG** | Retrieval + Augmentation + Generation | Grounds LLM responses in your data |\n",
    "| **Embeddings** | Vector representations of text | Enables semantic search |\n",
    "| **Vector DB** | Database optimized for high-dimensional vectors | Fast similarity search at scale |\n",
    "| **Chunking** | Splitting documents into smaller pieces | Critical for retrieval quality |\n",
    "| **Retrieval** | Finding relevant context for queries | Determines answer quality |\n",
    "\n",
    "---\n",
    "\n",
    "## üìñ How to Use This Notebook\n",
    "\n",
    "1. **Run cells in order** - Each section builds on the previous one\n",
    "2. **Read the markdown cells** - They contain important explanations\n",
    "3. **Experiment** - Try modifying parameters to see what happens\n",
    "4. **Test with your data** - Replace example documents with your own\n",
    "\n",
    "**Ready? Let's start by installing dependencies!**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Otherwise, install individually:\n",
    "%pip install langchain langchain-core langchain-openai langchain-community langchain-text-splitters chromadb pypdf python-dotenv -q\n",
    "\n",
    "# Verify installation\n",
    "import sys\n",
    "print(f\"Python version: {sys.version}\")\n",
    "\n",
    "# Verify key packages are installed\n",
    "try:\n",
    "    import langchain\n",
    "    import langchain_core\n",
    "    print(f\"‚úÖ LangChain version: {langchain.__version__}\")\n",
    "    print(\"‚úÖ All core packages installed successfully!\")\n",
    "except ImportError as e:\n",
    "    print(f\"‚ö†Ô∏è  Error: {e}\")\n",
    "    print(\"Please ensure all packages are installed correctly.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## üõ†Ô∏è Setup & Imports\n",
    "\n",
    "Let's import all the essential components we'll need throughout this notebook.\n",
    "right"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Standard library imports\n",
    "import os\n",
    "from pathlib import Path\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "# LangChain core imports\n",
    "from langchain_openai import OpenAIEmbeddings, ChatOpenAI\n",
    "from langchain_community.document_loaders import PyPDFLoader, TextLoader\n",
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "from langchain_community.vectorstores import Chroma\n",
    "from langchain_core.prompts import PromptTemplate, ChatPromptTemplate\n",
    "\n",
    "# Modern LangChain agent imports (latest pattern from LangChain docs)\n",
    "from langchain.agents import create_agent\n",
    "from langchain.tools import tool\n",
    "\n",
    "# Load environment variables - check current directory first, then parent\n",
    "current_dir = Path.cwd()\n",
    "env_path = current_dir / '.env'\n",
    "if env_path.exists():\n",
    "    load_dotenv(env_path)\n",
    "    print(f\"‚úÖ Loading .env from: {env_path}\")\n",
    "else:\n",
    "    # Try parent directory\n",
    "    parent_env = current_dir.parent / '.env'\n",
    "    if parent_env.exists():\n",
    "        load_dotenv(parent_env)\n",
    "        print(f\"‚úÖ Loading .env from: {parent_env}\")\n",
    "    else:\n",
    "        # Fallback to default behavior (searches current and parent directories)\n",
    "        load_dotenv()\n",
    "        print(\"‚ö†Ô∏è  No .env file found. Using default load_dotenv() search\")\n",
    "\n",
    "# Verify API key is set\n",
    "api_key = os.getenv(\"OPENAI_API_KEY\")\n",
    "if not api_key:\n",
    "    print(\"‚ö†Ô∏è  WARNING: OPENAI_API_KEY not found in environment variables!\")\n",
    "    print(\"Please create a .env file in this directory with your OpenAI API key:\")\n",
    "    print(f\"   {current_dir / '.env'}\")\n",
    "    print(\"\\nFormat: OPENAI_API_KEY=sk-...\")\n",
    "else:\n",
    "    # Validate API key format (basic check)\n",
    "    if not api_key.startswith('sk-'):\n",
    "        print(\"‚ö†Ô∏è  WARNING: API key format looks incorrect. Should start with 'sk-'\")\n",
    "    else:\n",
    "        print(\"‚úÖ OpenAI API key loaded successfully!\")\n",
    "        # Test the API key by making a simple call\n",
    "        try:\n",
    "            test_llm = ChatOpenAI(model=\"gpt-3.5-turbo\", temperature=0, max_tokens=5)\n",
    "            test_llm.invoke(\"Hi\")\n",
    "            print(\"‚úÖ API key validated successfully!\")\n",
    "        except Exception as e:\n",
    "            print(f\"‚ùå ERROR: API key validation failed!\")\n",
    "            print(f\"   Error: {str(e)}\")\n",
    "            print(\"\\nüí° Please check:\")\n",
    "            print(\"   1. Your API key is correct (get it from https://platform.openai.com/api-keys)\")\n",
    "            print(\"   2. You have sufficient credits in your OpenAI account\")\n",
    "            print(\"   3. The .env file is in the correct location\")\n",
    "            raise\n",
    "\n",
    "print(\"\\n‚úÖ All imports successful!\")\n",
    "print(\"üìö Using modern LangChain patterns: create_agent with @tool decorator\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## üéØ Part 1: Why RAG Exists\n",
    "\n",
    "### The Problem with LLMs\n",
    "\n",
    "LLMs are incredibly powerful, but they have critical limitations:\n",
    "\n",
    "1. **Knowledge Cutoff** - They only know what they were trained on (e.g., GPT-4 trained on data up to April 2023)\n",
    "2. **Hallucination** - They can confidently make up information\n",
    "3. **No Access to Private Data** - They can't access your documents, databases, or internal knowledge\n",
    "4. **Static Knowledge** - They can't learn new information after training\n",
    "\n",
    "### How RAG Solves This\n",
    "\n",
    "**RAG = Retrieval + Augmentation + Generation**\n",
    "\n",
    "1. **Retrieval**: Find relevant information from your knowledge base\n",
    "2. **Augmentation**: Add that information to the LLM's prompt\n",
    "3. **Generation**: LLM generates an answer grounded in the retrieved context\n",
    "\n",
    "This gives you:\n",
    "- ‚úÖ Up-to-date information\n",
    "- ‚úÖ Access to private data\n",
    "- ‚úÖ Reduced hallucination\n",
    "- ‚úÖ Source attribution\n",
    "\n",
    "### RAG vs Fine-Tuning vs Prompt Engineering\n",
    "\n",
    "| Approach | What It Does | Best For |\n",
    "|----------|-------------|----------|\n",
    "| **RAG** | Adds knowledge to prompts | New information, private data, facts |\n",
    "| **Fine-tuning** | Changes model behavior | Style, format, domain-specific tasks |\n",
    "| **Prompt Engineering** | Optimizes prompts | Simple tasks, no code changes needed |\n",
    "\n",
    "**Key insight:** RAG handles *knowledge*, fine-tuning handles *behavior* ‚Äî they're not either/or!\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## üèóÔ∏è Part 2: The RAG Architecture\n",
    "\n",
    "The RAG architecture has two main phases:\n",
    "\n",
    "### Phase 1: Indexing (One-time setup)\n",
    "\n",
    "1. **Load your knowledge base** - Documents, PDFs, text files, etc.\n",
    "2. **Split into chunks** - Break documents into manageable pieces\n",
    "3. **Convert to vectors** - Use an embedding model to create vector representations\n",
    "4. **Store in Vector DB** - Save vectors for fast retrieval\n",
    "\n",
    "### Phase 2: Query (Real-time)\n",
    "\n",
    "1. **User asks a question** - \"What are the key findings?\"\n",
    "2. **Embed the query** - Convert the question to a vector\n",
    "3. **Find similar chunks** - Search the vector database for relevant documents\n",
    "4. **Add context to prompt** - Combine retrieved chunks with the original question\n",
    "5. **LLM generates answer** - The model uses the context to answer\n",
    "6. **Return grounded response** - Answer with source attribution\n",
    "\n",
    "### Visual Flow\n",
    "\n",
    "```\n",
    "INDEXING:\n",
    "Knowledge Base ‚Üí Chunks ‚Üí Embedding Model ‚Üí Vector DB\n",
    "\n",
    "QUERY:\n",
    "User Query ‚Üí Embedding Model ‚Üí Vector DB (search) ‚Üí Top K Docs ‚Üí \n",
    "Augment Prompt ‚Üí LLM ‚Üí Output\n",
    "```\n",
    "\n",
    "Let's build this step by step!\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## üî¢ Part 3: Embeddings Deep Dive\n",
    "\n",
    "### What are Embeddings?\n",
    "\n",
    "Embeddings are **vector representations of text** that capture semantic meaning. Similar texts have similar vectors.\n",
    "\n",
    "### Key Properties\n",
    "\n",
    "1. **Semantic Similarity** - \"dog\" and \"puppy\" are close in vector space\n",
    "2. **Fixed Dimensions** - Each embedding has a fixed size (e.g., 1536 for OpenAI's text-embedding-3-small)\n",
    "3. **Distance = Meaning** - Closer vectors = more similar meaning\n",
    "\n",
    "### Why Embeddings Matter\n",
    "\n",
    "- **Traditional search**: \"Python programming\" won't match \"coding in Python\"\n",
    "- **Semantic search**: \"Python programming\" WILL match \"coding in Python\" because they're semantically similar\n",
    "\n",
    "Let's see embeddings in action:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the embedding model\n",
    "embeddings = OpenAIEmbeddings(model=\"text-embedding-3-small\")\n",
    "\n",
    "# Example texts to embed\n",
    "texts = [\n",
    "    \"Python is a programming language\",\n",
    "    \"Coding in Python is fun\",\n",
    "    \"Dogs are loyal pets\",\n",
    "    \"Cats are independent animals\"\n",
    "]\n",
    "\n",
    "# Generate embeddings\n",
    "text_embeddings = embeddings.embed_documents(texts)\n",
    "\n",
    "print(f\"Number of texts: {len(texts)}\")\n",
    "print(f\"Embedding dimensions: {len(text_embeddings[0])}\")\n",
    "print(f\"\\nFirst embedding (first 10 values): {text_embeddings[0][:10]}\")\n",
    "\n",
    "# Calculate similarity between first two texts\n",
    "import numpy as np\n",
    "\n",
    "def cosine_similarity(vec1, vec2):\n",
    "    \"\"\"Calculate cosine similarity between two vectors\"\"\"\n",
    "    return np.dot(vec1, vec2) / (np.linalg.norm(vec1) * np.linalg.norm(vec2))\n",
    "\n",
    "# Compare similarities\n",
    "similarity_01 = cosine_similarity(text_embeddings[0], text_embeddings[1])  # Python texts\n",
    "similarity_02 = cosine_similarity(text_embeddings[0], text_embeddings[2])  # Python vs Dog\n",
    "\n",
    "print(f\"\\nüìä Similarity Scores:\")\n",
    "print(f\"  'Python is a programming language' vs 'Coding in Python is fun': {similarity_01:.4f}\")\n",
    "print(f\"  'Python is a programming language' vs 'Dogs are loyal pets': {similarity_02:.4f}\")\n",
    "print(f\"\\n‚úÖ Notice: Similar topics have higher similarity scores!\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## ‚úÇÔ∏è Part 4: Chunking Strategies\n",
    "\n",
    "### Why Chunking Matters\n",
    "\n",
    "**Chunking is the make-or-break step in RAG.** Bad chunking = bad retrieval = bad answers.\n",
    "\n",
    "### The Challenge\n",
    "\n",
    "- **Too small**: Lose context, incomplete information\n",
    "- **Too large**: Noise, irrelevant information, token limits\n",
    "- **Wrong boundaries**: Split sentences, lose meaning\n",
    "\n",
    "### Chunking Strategies\n",
    "\n",
    "| Strategy | How It Works | Best For |\n",
    "|----------|-------------|----------|\n",
    "| **Fixed-size** | Split every N characters/tokens | Simple, predictable |\n",
    "| **Recursive** | Split by paragraphs ‚Üí sentences ‚Üí words | General-purpose (LangChain default) |\n",
    "| **Semantic** | Split when meaning shifts | High-quality but slower |\n",
    "| **Document-aware** | Split by headers, sections, pages | Structured docs (markdown, HTML) |\n",
    "\n",
    "### Best Practices\n",
    "\n",
    "1. **Use overlap** - 10-20% overlap prevents losing context at boundaries\n",
    "2. **Respect structure** - Don't split in the middle of sentences\n",
    "3. **Consider your use case** - Technical docs need larger chunks, Q&A needs smaller\n",
    "\n",
    "Let's see different chunking strategies:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sample document for chunking demonstration\n",
    "sample_document = \"\"\"\n",
    "Artificial Intelligence (AI) is transforming how we work and live. \n",
    "Machine learning, a subset of AI, enables computers to learn from data without explicit programming.\n",
    "\n",
    "Natural Language Processing (NLP) is another key area of AI. \n",
    "It allows computers to understand and generate human language.\n",
    "\n",
    "Large Language Models (LLMs) like GPT-4 represent a breakthrough in NLP. \n",
    "They can generate human-like text and understand context.\n",
    "\n",
    "Retrieval Augmented Generation (RAG) combines LLMs with information retrieval. \n",
    "This allows AI systems to answer questions using up-to-date information from knowledge bases.\n",
    "\"\"\"\n",
    "\n",
    "# Strategy 1: Recursive Character Text Splitter (Recommended)\n",
    "recursive_splitter = RecursiveCharacterTextSplitter(\n",
    "    chunk_size=150,\n",
    "    chunk_overlap=30,\n",
    "    separators=[\"\\n\\n\", \"\\n\", \". \", \" \", \"\"]\n",
    ")\n",
    "\n",
    "chunks_recursive = recursive_splitter.split_text(sample_document)\n",
    "\n",
    "print(\"üìÑ Original document length:\", len(sample_document), \"characters\")\n",
    "print(f\"\\n‚úÇÔ∏è  Recursive Splitter Results ({len(chunks_recursive)} chunks):\")\n",
    "print(\"-\" * 60)\n",
    "for i, chunk in enumerate(chunks_recursive, 1):\n",
    "    print(f\"\\nChunk {i} ({len(chunk)} chars):\")\n",
    "    print(f\"  {chunk[:100]}...\" if len(chunk) > 100 else f\"  {chunk}\")\n",
    "\n",
    "# Strategy 2: Fixed-size chunks (for comparison)\n",
    "fixed_splitter = RecursiveCharacterTextSplitter(\n",
    "    chunk_size=100,\n",
    "    chunk_overlap=0,\n",
    "    separators=[\"\"]\n",
    ")\n",
    "\n",
    "chunks_fixed = fixed_splitter.split_text(sample_document)\n",
    "\n",
    "print(f\"\\n\\n‚úÇÔ∏è  Fixed-size Splitter Results ({len(chunks_fixed)} chunks):\")\n",
    "print(\"-\" * 60)\n",
    "for i, chunk in enumerate(chunks_fixed[:3], 1):  # Show first 3\n",
    "    print(f\"\\nChunk {i} ({len(chunk)} chars):\")\n",
    "    print(f\"  {chunk[:100]}...\" if len(chunk) > 100 else f\"  {chunk}\")\n",
    "\n",
    "print(\"\\nüí° Notice how recursive splitter respects sentence boundaries better!\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## üóÑÔ∏è Part 5: Vector Databases\n",
    "\n",
    "### What is a Vector Database?\n",
    "\n",
    "A **vector database** is specialized storage optimized for:\n",
    "- High-dimensional vectors (hundreds to thousands of dimensions)\n",
    "- Fast similarity search (finding nearest neighbors)\n",
    "- Scalability (millions of vectors)\n",
    "\n",
    "### Why Not Regular Databases?\n",
    "\n",
    "Regular SQL databases are great for exact matches, but terrible for:\n",
    "- \"Find documents similar to this query\"\n",
    "- Semantic search\n",
    "- High-dimensional data\n",
    "\n",
    "### Vector Database Options\n",
    "\n",
    "| Database | Best For | Notes |\n",
    "|----------|----------|-------|\n",
    "| **ChromaDB** | Development, small-medium scale | Easy to use, Python-native |\n",
    "| **Pinecone** | Production, large scale | Managed service, fast |\n",
    "| **Weaviate** | Enterprise, complex queries | Open source, feature-rich |\n",
    "| **Qdrant** | Performance-critical | Fast, open source |\n",
    "| **Milvus** | Very large scale | Distributed, enterprise |\n",
    "\n",
    "For this session, we'll use **ChromaDB** - it's simple, works great for learning, and is perfect for production at small-medium scale.\n",
    "\n",
    "### How Vector Search Works\n",
    "\n",
    "1. Query is embedded ‚Üí becomes a vector\n",
    "2. Vector DB finds vectors with highest cosine similarity\n",
    "3. Returns top K most similar documents\n",
    "\n",
    "Let's set up ChromaDB:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a simple vector store with sample documents\n",
    "sample_docs = [\n",
    "    \"Python is a high-level programming language known for its simplicity.\",\n",
    "    \"Machine learning is a subset of artificial intelligence.\",\n",
    "    \"Vector databases enable semantic search over large text collections.\",\n",
    "    \"LangChain is a framework for building LLM applications.\",\n",
    "    \"RAG combines retrieval and generation for grounded AI responses.\"\n",
    "]\n",
    "\n",
    "# Split into chunks (we already covered chunking strategies in Part 4)\n",
    "text_splitter = RecursiveCharacterTextSplitter(\n",
    "    chunk_size=100,\n",
    "    chunk_overlap=20\n",
    ")\n",
    "\n",
    "# For this demo, we'll treat each sentence as a chunk\n",
    "chunks = sample_docs\n",
    "\n",
    "# Create embeddings and store in ChromaDB\n",
    "vectorstore = Chroma.from_texts(\n",
    "    texts=chunks,\n",
    "    embedding=embeddings,\n",
    "    persist_directory=\"./chroma_demo_db\"\n",
    ")\n",
    "\n",
    "print(f\"‚úÖ Created vector store with {len(chunks)} documents\")\n",
    "print(f\"üìÅ Database saved to: ./chroma_demo_db\")\n",
    "\n",
    "# Test similarity search\n",
    "query = \"What is Python?\"\n",
    "results = vectorstore.similarity_search(query, k=2)\n",
    "\n",
    "print(f\"\\nüîç Query: '{query}'\")\n",
    "print(f\"\\nüìÑ Top {len(results)} results:\")\n",
    "for i, doc in enumerate(results, 1):\n",
    "    print(f\"\\n  {i}. {doc.page_content}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## üõ†Ô∏è Part 6: Live Build - Document Q&A System\n",
    "\n",
    "Now let's build a complete RAG system step by step!\n",
    "\n",
    "### What We're Building\n",
    "\n",
    "A Document Q&A system that can:\n",
    "1. Load PDF or text documents\n",
    "2. Chunk and embed them\n",
    "3. Store in a vector database\n",
    "4. Answer questions based on the documents\n",
    "\n",
    "### Architecture\n",
    "\n",
    "```\n",
    "PDF/Text Files ‚Üí Load ‚Üí Chunk ‚Üí Embed ‚Üí Store in ChromaDB\n",
    "                                                      ‚Üì\n",
    "User Query ‚Üí Embed ‚Üí Search ‚Üí Retrieve Top K ‚Üí Augment Prompt ‚Üí LLM ‚Üí Answer\n",
    "```\n",
    "\n",
    "Let's build it!\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 1: Create Sample Document\n",
    "\n",
    "First, let's create a sample document to work with. In production, you'd load your own PDFs or text files.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a sample document file for demonstration\n",
    "sample_content = \"\"\"\n",
    "# Introduction to RAG Systems\n",
    "\n",
    "## What is RAG?\n",
    "\n",
    "Retrieval Augmented Generation (RAG) is a technique that enhances the capabilities of Large Language Models (LLMs) by providing them with access to external knowledge sources.\n",
    "\n",
    "## How RAG Works\n",
    "\n",
    "RAG systems work in two phases:\n",
    "\n",
    "### Phase 1: Indexing\n",
    "1. Documents are loaded from various sources (PDFs, databases, APIs)\n",
    "2. Documents are split into smaller chunks\n",
    "3. Each chunk is converted to a vector using an embedding model\n",
    "4. Vectors are stored in a vector database\n",
    "\n",
    "### Phase 2: Querying\n",
    "1. User asks a question\n",
    "2. Question is converted to a vector\n",
    "3. Vector database finds similar document chunks\n",
    "4. Retrieved chunks are added to the LLM prompt\n",
    "5. LLM generates an answer based on the retrieved context\n",
    "\n",
    "## Benefits of RAG\n",
    "\n",
    "- Reduces hallucination by grounding answers in real data\n",
    "- Allows access to up-to-date information\n",
    "- Enables use of private/internal documents\n",
    "- Provides source attribution for answers\n",
    "\n",
    "## Best Practices\n",
    "\n",
    "1. Use appropriate chunk sizes (typically 500-1000 characters)\n",
    "2. Include overlap between chunks (10-20%)\n",
    "3. Choose the right embedding model for your domain\n",
    "4. Test retrieval quality before optimizing generation\n",
    "5. Use metadata filtering when possible\n",
    "\"\"\"\n",
    "\n",
    "# Save to a text file\n",
    "with open(\"sample_rag_document.txt\", \"w\") as f:\n",
    "    f.write(sample_content)\n",
    "\n",
    "print(\"‚úÖ Created sample document: sample_rag_document.txt\")\n",
    "print(f\"üìÑ Document length: {len(sample_content)} characters\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 2: Load Documents\n",
    "\n",
    "LangChain supports 80+ document loaders for different formats.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the document\n",
    "loader = TextLoader(\"sample_rag_document.txt\")\n",
    "documents = loader.load()\n",
    "\n",
    "print(f\"‚úÖ Loaded {len(documents)} document(s)\")\n",
    "print(f\"üìÑ First document preview:\")\n",
    "print(\"-\" * 60)\n",
    "print(documents[0].page_content[:300] + \"...\")\n",
    "print(\"-\" * 60)\n",
    "print(f\"\\nüìä Document metadata: {documents[0].metadata}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 3: Chunk Documents\n",
    "\n",
    "Split documents into manageable chunks with overlap.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create text splitter\n",
    "text_splitter = RecursiveCharacterTextSplitter(\n",
    "    chunk_size=500,      # Size of each chunk\n",
    "    chunk_overlap=100,   # Overlap between chunks (important!)\n",
    "    separators=[\"\\n\\n\", \"\\n\", \". \", \" \", \"\"]  # Try to split at these boundaries\n",
    ")\n",
    "\n",
    "# Split documents into chunks\n",
    "chunks = text_splitter.split_documents(documents)\n",
    "\n",
    "print(f\"‚úÖ Split document into {len(chunks)} chunks\")\n",
    "print(f\"\\nüìä Chunk statistics:\")\n",
    "print(f\"  Average chunk size: {sum(len(c.page_content) for c in chunks) / len(chunks):.0f} characters\")\n",
    "print(f\"  Min chunk size: {min(len(c.page_content) for c in chunks)} characters\")\n",
    "print(f\"  Max chunk size: {max(len(c.page_content) for c in chunks)} characters\")\n",
    "\n",
    "print(f\"\\nüìÑ First chunk preview:\")\n",
    "print(\"-\" * 60)\n",
    "print(chunks[0].page_content)\n",
    "print(\"-\" * 60)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 4: Create Embeddings and Vector Store\n",
    "\n",
    "Convert chunks to vectors and store in ChromaDB.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize embeddings\n",
    "embeddings = OpenAIEmbeddings(model=\"text-embedding-3-small\")\n",
    "\n",
    "# Create vector store from chunks\n",
    "# This will:\n",
    "# 1. Generate embeddings for all chunks\n",
    "# 2. Store them in ChromaDB\n",
    "# 3. Persist to disk for later use\n",
    "\n",
    "vectorstore = Chroma.from_documents(\n",
    "    documents=chunks,\n",
    "    embedding=embeddings,\n",
    "    persist_directory=\"./rag_vector_db\"\n",
    ")\n",
    "\n",
    "print(\"‚úÖ Created vector store!\")\n",
    "print(f\"üìÅ Database saved to: ./rag_vector_db\")\n",
    "print(f\"üìä Stored {len(chunks)} document chunks\")\n",
    "\n",
    "# Test retrieval\n",
    "test_query = \"What are the benefits of RAG?\"\n",
    "results = vectorstore.similarity_search(test_query, k=1)\n",
    "\n",
    "print(f\"\\nüîç Test query: '{test_query}'\")\n",
    "print(f\"\\nüìÑ Retrieved {len(results)} relevant chunks:\")\n",
    "print(\"-\" * 60)\n",
    "for i, doc in enumerate(results, 1):\n",
    "    print(f\"\\n{i}. {doc.page_content[:200]}...\")\n",
    "    print(f\"   Metadata: {doc.metadata}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 5: Create the RAG Agent\n",
    "\n",
    "Now let's connect retrieval to the LLM using the modern LangChain agent pattern with tools. This is the recommended approach from the latest LangChain documentation.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize LLM\n",
    "model = ChatOpenAI(\n",
    "    model=\"gpt-4o-mini\",  # Fast and cost-effective\n",
    "    temperature=0  # Deterministic responses\n",
    ")\n",
    "\n",
    "# Create a retrieval tool using the modern @tool decorator pattern\n",
    "# This follows the latest LangChain documentation pattern\n",
    "@tool(response_format=\"content_and_artifact\")\n",
    "def retrieve_context(query: str):\n",
    "    \"\"\"Retrieve information from the document store to help answer a query.\"\"\"\n",
    "    retrieved_docs = vectorstore.similarity_search(query, k=3)\n",
    "    serialized = \"\\n\\n\".join(\n",
    "        (f\"Source: {doc.metadata}\\nContent: {doc.page_content}\")\n",
    "        for doc in retrieved_docs\n",
    "    )\n",
    "    return serialized, retrieved_docs\n",
    "\n",
    "# Create the RAG agent using create_agent (modern pattern)\n",
    "tools = [retrieve_context]\n",
    "system_prompt = (\n",
    "    \"You have access to a tool that retrieves context from documents. \"\n",
    "    \"Use the tool to help answer user queries. Always cite your sources.\"\n",
    ")\n",
    "\n",
    "rag_agent = create_agent(model, tools, system_prompt=system_prompt)\n",
    "\n",
    "print(\"‚úÖ RAG agent created using modern LangChain pattern!\")\n",
    "print(\"\\nüìã Agent configuration:\")\n",
    "print(f\"  LLM: {model.model_name}\")\n",
    "print(f\"  Retrieval: Top 3 chunks\")\n",
    "print(f\"  Pattern: Agent with @tool decorator (latest LangChain docs)\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 6: Ask Questions!\n",
    "\n",
    "Now let's test our RAG system with some questions.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test questions using the modern agent pattern\n",
    "questions = [\n",
    "    \"What is RAG?\",\n",
    "    \"How does RAG work?\",\n",
    "    \"What are the benefits of RAG?\",\n",
    "]\n",
    "\n",
    "for question in questions:\n",
    "    print(f\"\\n{'='*70}\")\n",
    "    print(f\"‚ùì Question: {question}\")\n",
    "    print(f\"{'='*70}\\n\")\n",
    "    \n",
    "    # Use agent.stream() with stream_mode=\"values\" (modern pattern)\n",
    "    for event in rag_agent.stream(\n",
    "        {\"messages\": [{\"role\": \"user\", \"content\": question}]},\n",
    "        stream_mode=\"values\",\n",
    "    ):\n",
    "        # Get the last message from the event\n",
    "        last_message = event[\"messages\"][-1]\n",
    "        \n",
    "        # Display tool calls if present\n",
    "        if hasattr(last_message, 'tool_calls') and last_message.tool_calls:\n",
    "            print(\"üîß Tool Calls:\")\n",
    "            for tool_call in last_message.tool_calls:\n",
    "                print(f\"  - {tool_call.get('name', 'unknown')}: {tool_call.get('args', {})}\")\n",
    "            print()\n",
    "        \n",
    "        # Display the final answer\n",
    "        if hasattr(last_message, 'content') and last_message.content:\n",
    "            print(f\"üí¨ Answer:\\n{last_message.content}\\n\")\n",
    "    \n",
    "    print(\"-\"*70)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## üß™ Part 7: Evaluation & Debugging\n",
    "\n",
    "### Common RAG Failure Modes\n",
    "\n",
    "| Problem | Symptom | Fix |\n",
    "|---------|---------|-----|\n",
    "| **Bad chunking** | Half-relevant text | Adjust chunk size, use semantic splitting |\n",
    "| **Wrong K value** | Too much noise or missing context | Experiment with K=3 to K=10 |\n",
    "| **Embedding mismatch** | Irrelevant results | Try different embedding model |\n",
    "| **Prompt leakage** | LLM ignores context | Strengthen grounding instructions |\n",
    "| **Boundary issues** | Answer split across chunks | Increase chunk overlap |\n",
    "\n",
    "### Testing Your RAG System\n",
    "\n",
    "1. **Test retrieval first** - If retrieval is bad, generation will be bad\n",
    "2. **Check chunk quality** - Are chunks meaningful and complete?\n",
    "3. **Verify embedding quality** - Do similar queries return similar results?\n",
    "4. **Evaluate answers** - Are answers grounded in the context?\n",
    "\n",
    "Let's create a simple evaluation function:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_retrieval(vectorstore, query, k=3):\n",
    "    \"\"\"Evaluate retrieval quality by showing retrieved chunks\"\"\"\n",
    "    results = vectorstore.similarity_search(query, k=k)\n",
    "    \n",
    "    print(f\"üîç Query: '{query}'\")\n",
    "    print(f\"üìä Retrieved {len(results)} chunks\\n\")\n",
    "    \n",
    "    for i, doc in enumerate(results, 1):\n",
    "        relevance_score = \"‚úÖ\" if any(keyword in doc.page_content.lower() \n",
    "                                     for keyword in query.lower().split()) else \"‚ö†Ô∏è\"\n",
    "        print(f\"{relevance_score} Chunk {i} ({len(doc.page_content)} chars):\")\n",
    "        print(f\"   {doc.page_content[:200]}...\")\n",
    "        print()\n",
    "    \n",
    "    return results\n",
    "\n",
    "# Test retrieval quality\n",
    "print(\"=\"*70)\n",
    "print(\"RETRIEVAL QUALITY TEST\")\n",
    "print(\"=\"*70)\n",
    "evaluate_retrieval(vectorstore, \"What are the benefits of RAG?\", k=3)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## üéì Summary & Next Steps\n",
    "\n",
    "### What We Built\n",
    "\n",
    "‚úÖ Complete RAG system with:\n",
    "- Document loading\n",
    "- Intelligent chunking\n",
    "- Vector embeddings\n",
    "- Vector database storage\n",
    "- Retrieval-augmented Q&A\n",
    "- Evaluation tools\n",
    "\n",
    "### Key Takeaways\n",
    "\n",
    "1. **RAG = Retrieval + Augmentation + Generation**\n",
    "2. **Chunking is critical** - Bad chunking = bad results\n",
    "3. **Test retrieval before generation** - Garbage in, garbage out\n",
    "4. **Embeddings enable semantic search** - Similar meaning = similar vectors\n",
    "5. **Vector databases scale** - Fast similarity search at scale\n",
    "\n",
    "### Next Steps\n",
    "\n",
    "1. **Try with your own documents** - Replace the sample document with your PDFs/text files\n",
    "2. **Experiment with chunk sizes** - Find what works best for your data\n",
    "3. **Try different embedding models** - Some models work better for specific domains\n",
    "4. **Add metadata filtering** - Filter by date, source, category before retrieval\n",
    "5. **Implement hybrid search** - Combine vector search with keyword matching\n",
    "6. **Add reranking** - Use a cross-encoder to improve top results\n",
    "\n",
    "### Resources\n",
    "\n",
    "- **LangChain Docs**: https://python.langchain.com/\n",
    "- **ChromaDB Docs**: https://docs.trychroma.com/\n",
    "- **OpenAI Embeddings**: https://platform.openai.com/docs/guides/embeddings\n",
    "- **RAG Evaluation**: Consider using RAGAS (https://docs.ragas.io/)\n",
    "\n",
    "### Your Challenge\n",
    "\n",
    "Build a RAG system on YOUR data this week! Start simple, iterate, and remember:\n",
    "- **Start simple** - Basic RAG with good chunking beats complex RAG with bad data\n",
    "- **Test retrieval first** - If retrieval is bad, nothing else matters\n",
    "- **Iterate** - RAG is an iterative process, not a one-time setup\n",
    "\n",
    "---\n",
    "\n",
    "## üéâ Congratulations!\n",
    "\n",
    "You've built a complete RAG system! You now have the foundation to build production-ready AI applications that can answer questions based on your own data.\n",
    "\n",
    "**Happy building! üöÄ**\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
